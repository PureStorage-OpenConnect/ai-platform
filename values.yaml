custom: {}

hub:
  service:
    type: ClusterIP
    annotations: {}
    ports:
      nodePort:
    loadBalancerIP:
  baseUrl: /
  cookieSecret:
  publicURL:
  uid: 1000
  fsGid: 1000
  nodeSelector: {}
  concurrentSpawnLimit: 64
  consecutiveFailureLimit: 5
  activeServerLimit:
  deploymentStrategy:
    # sqlite-pvc backed hub requires Recreate strategy to work
    type: Recreate
    # This is required for upgrading to work
    rollingUpdate:
  db:
    type: sqlite-pvc
    upgrade:
    pvc:
      annotations: {}
      selector: {}
      accessModes:
        - ReadWriteOnce
      storage: 1Gi
      subPath:
      storageClassName: pure-file
    url:
    password:
  labels: {}
  annotations: {}
  extraConfig: 
    jupyterlab: |
      c.Spawner.cmd = ['jupyter-labhub']
  extraConfigMap: {}
  extraEnv: {}
  extraContainers: []
  extraVolumes: []
  extraVolumeMounts: []
  image:
    name: jupyterhub/k8s-hub
    tag: '0.8.2'
  resources:
    requests:
      cpu: 200m
      memory: 512Mi
  services: {}
  imagePullPolicy: IfNotPresent
  imagePullSecret:
    enabled: false
    registry:
    username:
    email:
    password:
  pdb:
    enabled: true
    minAvailable: 1
  networkPolicy:
    enabled: false
    egress:
      - to:
          - ipBlock:
              cidr: 0.0.0.0/0
  allowNamedServers: false


rbac:
  enabled: true


proxy:
  secretToken: 'SECRET_TOKEN'   # insert 32-digit secret token (use straight quotes '') 
  service:
    type: LoadBalancer
    labels: {}
    annotations: {}
    nodePorts:
      http:
      https:
    loadBalancerIP:
  chp:
    image:
      name: jupyterhub/configurable-http-proxy
      tag: 3.0.0
      pullPolicy: IfNotPresent
    resources:
      requests:
        cpu: 200m
        memory: 512Mi
  nginx:
    image:
      name: quay.io/kubernetes-ingress-controller/nginx-ingress-controller
      tag: 0.15.0
      pullPolicy: IfNotPresent
    proxyBodySize: 64m
    resources: {}
  lego:
    image:
      name: jetstack/kube-lego
      tag: 0.1.7
      pullPolicy: IfNotPresent
    resources: {}
  labels: {}
  nodeSelector: {}
  pdb:
    enabled: true
    minAvailable: 1
  https:
    enabled: true
    type: letsencrypt
    #type: letsencrypt, manual, offload, secret
    letsencrypt:
      contactEmail: ''
    manual:
      key:
      cert:
    secret:
      name: ''
      key: tls.key
      crt: tls.crt
    hosts: []
  networkPolicy:
    enabled: false
    egress:
      - to:
          - ipBlock:
              cidr: 0.0.0.0/0


auth:
  type: dummy
  whitelist:
    users:
  admin:
    access: true
    users:
  dummy:
    password:
  ldap:
    dn:
      search: {}
      user: {}
    user: {}
  state:
    enabled: false
    cryptoKey:


singleuser:
  extraTolerations: []
  nodeSelector: {}
  extraNodeAffinity:
    required: []
    preferred: []
  extraPodAffinity:
    required: []
    preferred: []
  extraPodAntiAffinity:
    required: []
    preferred: []
  networkTools:
    image:
      name: jupyterhub/k8s-network-tools
      tag: '0.8.2'
  cloudMetadata:
    enabled: false
    ip: 169.254.169.254
  networkPolicy:
    enabled: false
    egress:
    # Required egress is handled by other rules so it's safe to modify this
      - to:
          - ipBlock:
              cidr: 0.0.0.0/0
              except:
                - 169.254.169.254/32
  events: true
  extraAnnotations: {}
  extraLabels:
    hub.jupyter.org/network-access-hub: 'true'
  extraEnv: 
    EDITOR: "vim"
  lifecycleHooks: []
  initContainers: []
  extraContainers: []
  uid: 1000
  fsGid: 100
  serviceAccountName:
  storage:
    type: dynamic
    extraLabels: {}
    extraVolumes:   # access to shared datasets saved on FlashBlade 
      - name: shared-datasets
        persistentVolumeClaim: 
          claimName: shared-ai-datasets   # persistent volume claim created in the cluster
          storageClass: ""
    extraVolumeMounts: 
      - name: shared-datasets
        mountPath: /home/shared   # mount path for shared datasets with each user's environment 
    static:
      pvcName: 
      subPath: /home/{username}   # user home directory
      type: static
      uid: 0
    capacity: 10Gi
    homeMountPath: /home/jovyan
    dynamic:   # set FlashBlade as the storage target for PVCs of user environments 
      storageClass: pure-file   # storage class created in the cluster
      pvcNameTemplate: claim-{username}{servername}
      volumeNameTemplate: volume-{username}{servername}
      storageAccessModes: [ReadWriteMany]
  image:
    # custom image that contains a set of pre-installed libraries and tools
    name: emilypotyraj/pure-k8s-singleuser
    tag: latest
    pullPolicy: IfNotPresent
  imagePullSecret:
    enabled: false
    registry:
    username:
    email:
    password:
  startTimeout: 300
  cpu:
    limit:
    guarantee:
  memory:
    limit:
    guarantee: 4G
  extraResource:
    limits: {}
    guarantees: {}
  cmd: jupyterhub-singleuser
  defaultUrl: "/lab"   # switch default view from "tree" to "lab"


scheduling:
  userScheduler:
    enabled: false
    replicas: 1
    logLevel: 4
    image:
      name: gcr.io/google_containers/kube-scheduler-amd64
      tag: v1.11.2
    nodeSelector: {}
    pdb:
      enabled: true
      minAvailable: 1
    resources:
      requests:
        cpu: 50m
        memory: 256Mi
  podPriority:
    enabled: false
    globalDefault: false
    defaultPriority: 0
    userPlaceholderPriority: -10
  userPlaceholder:
    enabled: true
    replicas: 0
  corePods:
    nodeAffinity:
      matchNodePurpose: prefer
  userPods:
    nodeAffinity:
      matchNodePurpose: prefer


prePuller:
  hook:
    enabled: false
    image:
      name: jupyterhub/k8s-image-awaiter
      tag: '0.8.2'
  continuous:
    enabled: false
  extraImages: {}
  pause:
    image:
      name: gcr.io/google_containers/pause
      tag: '3.0'


ingress:
  enabled: false
  annotations: {}
  hosts: []
  pathSuffix: ''
  tls: []


cull:
  enabled: true
  users: false
  timeout: 600   # every server idle for > 10 min will be shut down
  every: 60   # check for inactivity every 60 sec
  concurrency: 10
  maxAge: 0


debug:
  enabled: false
